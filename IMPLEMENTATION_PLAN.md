# Implementation Plan: Phase 4-5 Features

**Date**: 2025-01-XX
**Status**: Ready for Implementation
**Scope**: Redlock, Translation, Likes, Analytics, Trending, Recommendations

---

## Overview

This plan covers the implementation of:

1. **Redlock** for upload finalize race condition protection
2. **Comment Translation Pipeline** with async processing
3. **Video Likes System** with Redis hot counters
4. **Analytics Service** using TimescaleDB
5. **Trending Section** based on engagement metrics
6. **Recommendation Engine** using GenAI

---

## 1. Redlock for Upload Finalize (NestJS)

### 1.1 Add Redis Lock Methods to CacheService

**File**: `nest-backend/src/database/redis/redis.service.ts`

- Add `acquireLock(key: string, ttlSeconds: number): Promise<boolean>` - Uses Redis SETNX with expiration
- Add `releaseLock(key: string): Promise<void>` - Deletes lock key
- Add `extendLock(key: string, ttlSeconds: number): Promise<boolean>` - Extends lock expiration
- Lock key format: `upload:finalize:{upload_id}`

### 1.2 Modify UploadService.completeMultipartUpload

**File**: `nest-backend/src/modules/videos/services/upload/upload.service.ts`

- Acquire lock before S3 `CompleteMultipartUploadCommand`
- Lock TTL: 60 seconds (sufficient for S3 operation)
- Check upload status (COMPLETED) before finalizing (idempotency)
- Release lock after completion or on error
- Handle lock acquisition failure gracefully (return appropriate error)
- Make operation idempotent: if upload already COMPLETED, return success without S3 call

### 1.3 Add Constants

**File**: `nest-backend/src/modules/videos/constants/upload.constants.ts` (create if needed)

- `UPLOAD_FINALIZE_LOCK_TTL = 60` (seconds)
- `UPLOAD_FINALIZE_LOCK_PREFIX = 'upload:finalize:'`

---

## 2. Comment Translation Pipeline

### 2.1 Database Schema Changes

**File**: `nest-backend/src/database/postgres/entities/comment.entity.ts`

- Add `translated_text: string | null` column (nullable, only set if not English)
- Migration will be auto-generated by TypeORM synchronize

### 2.2 Modify Comment Creation to Emit Event

**File**: `nest-backend/src/modules/videos/services/comments/comment.service.ts`

- Inject `OutboxService` and `DataSource`
- Modify `createComment()` to use transaction
- In transaction: create comment + add `comment.created` event to outbox
- Event payload: `{ id: uuid, commentId: number, videoId: number, content: string, userId: number, ts: ISO string }`

### 2.3 Create Comment Translation Module (Python)

**Structure**: `python-backend/modules/translation/`

#### 2.3.1 Service: Comment Translation Service

**File**: `python-backend/modules/translation/services/comment_translation_service.py`

- `detect_language(text: str) -> str`: Use OpenAI Whisper model (reuse from transcription module)
- `translate_text(text: str, target_lang: str = 'en') -> str`: Use OpenAI API (gpt-3.5-turbo or gpt-4)
- `get_cached_translation(text_hash: str) -> str | None`: Redis lookup
- `cache_translation(text_hash: str, translation: str) -> None`: Redis store with TTL
- `translate_comment(comment_id: int, content: str) -> Dict`: Main method that orchestrates detection + translation + cache + DB update

#### 2.3.2 Kafka Worker

**File**: `python-backend/modules/translation/kafka_worker.py`

- Consumer for `comment.created` topic
- Uses idempotency check via `ProcessedMessageService`
- Queues Celery task `translate_comment_task` if comment not already processed
- Follows same pattern as `summary/kafka_worker.py`

#### 2.3.3 Celery Task

**File**: `python-backend/modules/translation/tasks/comment_translation.py`

- Task name: `tasks.translate_comment`
- Base: `DatabaseTask` (reuse from summary/transcription)
- Payload: `{ id: event_id, commentId: number, content: string, videoId: number }`
- Retry logic: Exponential backoff (same config pattern as transcription)
- Calls `CommentTranslationService.translate_comment()`
- Updates comment in database with `translated_text` (only if not English)
- Updates comment via SQLAlchemy repository (need to add comment model to Python backend)

#### 2.3.4 Update Comment Model (Python)

**File**: `python-backend/database/models.py`

- Add `Comment` model (if not exists) with `translated_text: Column(String, nullable=True)`
- Ensure foreign key to `users` and `videos` tables

#### 2.3.5 Constants

**File**: `python-backend/modules/translation/constants.py`

- `TRANSLATION_TASK_CONFIG`: max_retries, initial_retry_delay (match transcription config)
- `TRANSLATION_CACHE_TTL = 86400` (24 hours)
- `TRANSLATION_CACHE_PREFIX = 'comment:translation:'`

### 2.4 Update Main Application

**File**: `python-backend/run_all.py`

- Add Kafka consumer for `comment.created` topic
- Register translation task with Celery app
- Follow same pattern as summary and transcription workers

### 2.5 Update Dependencies

**File**: `python-backend/requirements.txt`

- No new dependencies needed (reuse OpenAI, Whisper, Redis)

### 2.6 Optional: Emit comment.translated Event

- After successful translation, add `comment.translated` event to outbox
- Event payload: `{ id: uuid, commentId: number, originalLang: str, translatedText: str, ts: ISO string }`
- Future: Could be consumed by other services (e.g., moderation, search indexing)

---

## 3. Video Likes System

### 3.1 Database Schema

**File**: `nest-backend/src/database/postgres/entities/video-like.entity.ts` (create new)

- Entity: `VideoLike` with `user_id`, `video_id`
- Unique constraint: `['user_id', 'video_id']` (one like per user per video)
- Indexes: `['user_id']`, `['video_id']`
- Add to `TableNames` enum: `VIDEO_LIKES = 'video_likes'`

**File**: `nest-backend/src/modules/videos/entities/video.entity.ts`

- Add `likes: number` column (default 0) for like count
- Note: This is a denormalized counter, synced from Redis hot counter

### 3.2 Video Likes Service

**File**: `nest-backend/src/modules/videos/services/likes/video-like.service.ts` (create new)

- `toggleLikeVideo(videoId: number, userId: number)`: Toggle like/unlike
  - Use transaction for atomicity
  - Check if like exists (unique constraint prevents duplicates)
  - Atomic increment/decrement in Redis: `video:likes:{video_id}` (INCRBY)
  - Create/delete `VideoLike` record in transaction
  - Return: `{ hasLiked: boolean, likesCount: number }`
- `hasUserLikedVideo(videoId: number, userId: number): Promise<boolean>`
- `getVideoLikesCount(videoId: number): Promise<number>` - Check Redis first, fallback to DB

### 3.3 Redis Hot Counter for Likes

**File**: `nest-backend/src/modules/videos/services/likes/video-likes-counter.service.ts` (create new)

- `incrementLikes(videoId: number): Promise<number>` - Redis INCRBY
- `decrementLikes(videoId: number): Promise<number>` - Redis DECRBY
- `getLikesCount(videoId: number): Promise<number>` - Redis GET, fallback to DB
- Counter key: `video:likes:{video_id}`
- Periodic flush: Scheduled job to sync Redis counters to DB (every 5 minutes)

### 3.4 Scheduled Counter Flush Job

**File**: `nest-backend/src/modules/videos/schedulers/video-likes-flush.scheduler.ts` (create new)

- Runs every 5 minutes (configurable)
- Scans Redis keys matching `video:likes:*`
- For each key:
  - Get Redis count
  - Get DB count
  - If different, update DB atomically: `UPDATE videos SET likes = :count WHERE id = :videoId`
  - Reset Redis counter to 0 (or keep delta for next flush)
- Uses Lua script for atomic get-and-reset if needed

### 3.5 Video Likes Controller

**File**: `nest-backend/src/modules/videos/controllers/video-likes.controller.ts` (create new)

- `POST /videos/:videoId/like` - Toggle like (uses service)
- `GET /videos/:videoId/like` - Check if user liked (optional)
- `GET /videos/liked` - Get user's liked videos (paginated)
  - Query: Join `video_likes` with `videos` table
  - Filter by `user_id`
  - Order by `created_at DESC` (newest first)

### 3.6 Update Video Info to Include Likes

**File**: `nest-backend/src/modules/videos/services/video-info.service.ts`

- Modify `getVideoInfo()` to include `likes` count (from Redis hot counter)
- Include `has_liked: boolean` if userId provided

---

## 4. Analytics Service

### 4.1 Time Series Database Setup

**Technology Choice**: TimescaleDB (PostgreSQL extension) - leverages existing PostgreSQL infrastructure

**Alternative**: InfluxDB (separate service) - if you prefer dedicated time-series DB

**File**: `nest-backend/src/database/timescale/analytics-schema.sql` (create migration)

- Create hypertable for video analytics: `video_analytics`
- Columns: `video_id`, `user_id` (nullable), `event_type`, `timestamp`, `metadata` (JSONB)
- Event types: `view`, `like`, `comment`, `watch_time`, `share`
- Partition by time (daily or hourly chunks)

### 4.2 Analytics Service

**File**: `nest-backend/src/modules/analytics/services/analytics.service.ts` (create new)

- `trackEvent(event: AnalyticsEvent)`: Insert into TimescaleDB
- `getVideoStats(videoId: number, timeRange: TimeRange)`: Aggregate queries
- `getTrendingVideos(timeRange: TimeRange, limit: number)`: Trending algorithm

### 4.3 Kafka Events for Analytics

**File**: `nest-backend/src/modules/analytics/services/analytics-kafka-consumer.service.ts` (create new)

- Consume events: `video.viewed`, `video.liked`, `video.unliked`, `comment.created`, etc.
- Transform to analytics events
- Batch insert to TimescaleDB (e.g., every 100 events or 5 seconds)
- Use connection pooling for TimescaleDB

### 4.4 Analytics Events from Existing Services

**File**: `nest-backend/src/modules/videos/services/watch/watch.service.ts`

- Emit `video.viewed` event after video watch URL generated (if not already tracked)
- Event payload: `{ id: uuid, videoId: number, userId: number | null, timestamp: ISO string }`

**File**: `nest-backend/src/modules/videos/services/likes/video-like.service.ts`

- Emit `video.liked` / `video.unliked` events via outbox
- Event payload: `{ id: uuid, videoId: number, userId: number, action: 'like' | 'unlike', ts: ISO string }`

**File**: `nest-backend/src/modules/videos/services/comments/comment.service.ts`

- Already emits `comment.created` - analytics service will consume this

### 4.5 Analytics Module

**File**: `nest-backend/src/modules/analytics/analytics.module.ts` (create new)

- Import TimescaleDB connection
- Register `AnalyticsService`, `AnalyticsKafkaConsumerController`
- Subscribe to Kafka topics: `video.viewed`, `video.liked`, `video.unliked`, `comment.created`

---

## 5. Trending Section

### 5.1 Trending Algorithm

**File**: `nest-backend/src/modules/analytics/services/trending.service.ts` (create new)

- Score formula (configurable weights):
  - `score = (views * 1.0) + (likes * 5.0) + (comments * 3.0) + (watch_time_minutes * 0.1) - (age_penalty)`
  - Age penalty: Older videos decay over time
  - Time window: Last 7 days, 30 days, or all time
- `getTrendingVideos(timeRange: TimeRange, limit: number, category?: string)`: Query TimescaleDB
  - Use SQL window functions for ranking
  - Consider recency bias (recent activity weighted higher)

### 5.2 Trending Controller

**File**: `nest-backend/src/modules/analytics/controllers/trending.controller.ts` (create new)

- `GET /analytics/trending`: Get trending videos
  - Query params: `timeRange` (7d, 30d, all), `limit`, `category` (optional)
  - Returns: Array of videos with trending scores

### 5.3 Caching Trending Results

**File**: `nest-backend/src/modules/analytics/services/trending.service.ts`

- Cache trending results in Redis (TTL: 5 minutes)
- Cache key: `trending:{timeRange}:{limit}`
- Invalidate on new analytics events (async, non-blocking)

---

## 6. Recommendation Engine (GenAI)

### 6.1 Recommendation Service (Python)

**File**: `python-backend/modules/recommendations/services/video_recommendation_service.py` (create new)

- `generateRecommendations(userId: int, limit: int = 10) -> List[int]`: Generate video recommendations
- Input context:
  - User's watch history (last 20 videos)
  - User's liked videos (top 10)
  - Trending videos (top 20)
  - Video metadata: titles, descriptions, summaries, categories
- Prompt engineering:
  - System: "You are a video recommendation engine. Analyze user preferences and suggest relevant videos."
  - User: JSON with watch history, liked videos, trending videos, user profile
  - Response: List of video IDs with reasoning

### 6.2 Recommendation API Endpoint (Python)

**File**: `python-backend/modules/recommendations/routers/recommendations.py` (create new)

- `GET /recommendations?userId={id}&limit={n}`: Get recommendations
- Authentication: Validate JWT token (reuse auth from NestJS)
- Cache recommendations in Redis (TTL: 1 hour per user)

### 6.3 Recommendation Caching Strategy

**File**: `python-backend/modules/recommendations/services/video_recommendation_service.py`

- Cache key: `user:recommendations:{userId}`
- TTL: 1 hour (recommendations refresh hourly)
- Cache invalidation: When user likes/watches new videos (optional, can rely on TTL)

### 6.4 Recommendation Module Registration

**File**: `python-backend/main.py`

- Register recommendations router
- Add to FastAPI app

### 6.5 Frontend Integration (NestJS Proxy)

**File**: `nest-backend/src/modules/videos/controllers/recommendations.controller.ts` (optional)

- Proxy endpoint: `GET /videos/recommendations`
- Calls Python backend recommendation API
- Returns list of video IDs, then fetch full video details from NestJS

---

## Implementation Notes

### Redis Lock Strategy

- Lock key: `upload:finalize:{upload_id}`
- Acquire lock with SETNX + EX (atomic operation)
- If lock exists, wait briefly or return error (client should retry)
- Lock auto-expires after 60 seconds (failsafe)
- Always release lock in finally block

### Translation Caching Strategy

- Cache key: `comment:translation:{hash(content)}`
- Hash content to normalize (lowercase, trim whitespace)
- TTL: 24 hours (translations rarely change)
- Check cache before calling OpenAI API
- Store translation in cache after successful translation

### Video Likes Race Condition Handling

- Use Redis atomic INCRBY/DECRBY for hot path
- Unique constraint on `(user_id, video_id)` prevents duplicate likes
- Transaction ensures atomicity between Redis and DB
- Periodic flush syncs Redis counters to DB (every 5 minutes)
- On counter flush failure, Redis continues serving reads (eventual consistency)

### Analytics Data Model

- TimescaleDB hypertable: `video_analytics(video_id, user_id, event_type, timestamp, metadata)`
- Event types: `view`, `like`, `comment`, `watch_time`, `share`
- Batch inserts (every 100 events or 5 seconds) for performance
- Indexes: `(video_id, timestamp)`, `(event_type, timestamp)`, `(user_id, timestamp)`

### Trending Algorithm Details

- Score formula weights are configurable via environment variables
- Time decay: Exponential decay for older videos
- Recency boost: Recent events weighted 2x
- Category filtering: Optional category-based trending

### Recommendation Engine Approach

- Use GPT-4 or GPT-3.5-turbo for recommendations
- Input: User context (watch history, likes, trending) + video metadata
- Output: Ranked list of video IDs with brief reasoning
- Fallback: If GenAI fails, return trending videos as fallback
- Cost optimization: Cache recommendations, batch processing possible

### Idempotency

- Upload finalize: Check `UploadStatus.COMPLETED` before S3 call
- Comment translation: Use `processed_messages` table (existing pattern)
- Video likes: Unique constraint + Redis atomic ops
- Analytics events: Deduplicate by event ID if needed

### Error Handling

- Lock acquisition failure: Return 409 Conflict or 503 Service Unavailable
- Translation failure: Log error, don't fail comment creation (graceful degradation)
- Redis unavailable: Fallback to direct DB queries for likes
- Analytics DB unavailable: Log events to Kafka DLQ, retry later
- Recommendation API failure: Return trending videos as fallback

---

## Task Dependencies

1. **Redlock Implementation**
   - Add Redis lock methods → Modify upload service

2. **Translation Pipeline**
   - Update comment entity → Modify comment service → Create Python translation module

3. **Video Likes**
   - Create video_likes table → Create likes service → Create counter service → Create flush scheduler → Create controller

4. **Analytics**
   - Setup TimescaleDB → Create analytics service → Create Kafka consumer → Integrate with existing services

5. **Trending**
   - Analytics service must exist → Create trending service → Create controller

6. **Recommendations**
   - Analytics/trending must exist → Create recommendation service (Python) → Create API endpoint → Optional NestJS proxy

---

## Estimated Implementation Order

### Phase 1: Foundation (Week 1)

1. Redlock for upload finalize
2. Comment translation pipeline
3. Video likes system

### Phase 2: Analytics & Trending (Week 2)

4. Analytics service setup
5. Trending algorithm
6. Event integration

### Phase 3: Recommendations (Week 3)

7. Recommendation engine
8. API integration
9. Testing & optimization

---

## Testing Considerations

- **Redlock**: Test concurrent finalize requests (should only one succeed)
- **Translation**: Test caching (second identical comment should use cache), language detection accuracy
- **Video Likes**: Test race conditions (concurrent likes from same user), counter flush accuracy
- **Analytics**: Test batch insert performance, query performance with large datasets
- **Trending**: Test score calculation accuracy, caching effectiveness
- **Recommendations**: Test recommendation quality, fallback mechanism, cache behavior
